{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment 6\n",
    "\n",
    "### Due: February 19th, 2021, 11:59pm (Pacific Time).\n",
    "\n",
    "### Instructions: \n",
    "\n",
    "This HW is an experimental Jupyter notebook format. Please let us know if you like it or prefer the other way!\n",
    "\n",
    "### Problem 1 : \n",
    "Your math can be handwritten. In that case take a photo, put jpg image in the same directory as this notebook and put `![Alt text description](/path/to/file.jpg) ` in a Markdown cell under the question cell.  If you have questions about Markdown syntax just use your search engine for a cheat sheet. \n",
    "\n",
    "Alternatively, your math may be done directly in LaTeX commands within the Markdown cell. Just type something like `$\\frac{1}{2} {\\lVert \\mathbf{w} \\rVert_2}^2$` for $\\frac{1}{2} {\\lVert \\mathbf{w} \\rVert_2}^2$.  Also syntax like `$$ ... math stuff .. $$` produces an equation on its own line instead of wedged into the current text like the single \\$ version does.\n",
    "\n",
    "### Problems 2-8:\n",
    "Output from Jupyter Notebook code.  *Make sure you don't write long lines of code that don't linewrap*. We need to be able to see your code when it gets turned into PDF below.\n",
    "\n",
    "### Submission:\n",
    "Generate a PDF by using `File->Print->Preview` or `File->Download as->PDF via LaTeX`. Please make sure and keep your ipynb file... we may ask you to submit it if we have questions about your code.\n",
    "\n",
    "### Late Policy:\n",
    "Late assignments are deducted 5\\% each day they are late. No late assignments are accepted after one week. \n",
    "\n",
    "### System Setup: \n",
    "For this class, please use \\textbf{Python 3.6} or later for homework with recent copies of the libraries NumPy, SciPy, Pandas, Scikit-learn, Matplotlib, Seaborn, and Jupyter-Notebook. \n",
    "\n",
    "To run a Jupyter Notebook you may use [UCSD Datahub](https://datahub.ucsd.edu) or [Google Colab](https://colab.google.com) or your own local installation.  If you decide on local installation, and you are creating your own setup for the first time we highly recommend you use [Anaconda](https://anaconda.com) as it will come with all the required libraries and more.  \n",
    "\n",
    "If you are not feeling comfortable with the programming assignments in this homework, it might help to take a look at https://github.com/UCSD-COGS108/Tutorials\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression\n",
    "\n",
    "Assume in a binary classification problem, we need to predict a binary label $y\\in \\{-1,+1\\}$ for a feature vector $\\mathbf{x}=[x_0,x_1]^\\top$. In logistic regression, we can reformulate the binary classification problem in a probabilistic framework: We aim to model the distribution of classes given the input feature vector $\\mathbf{x}$. Specifically, we can express the conditional probability $p(y|\\mathbf{x})$ parameterized by $(\\mathbf w, b)$ using a logistic function. Assume the probability of the positive prediction $p(y=+1|\\mathbf{x})$ is represented as:\n",
    "\\begin{align}\n",
    "p(y=+1|\\mathbf{x})=\\displaystyle\\frac{1}{1+e^{-(\\mathbf w^T \\mathbf{x} + b)}}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "## 1.1 (10 points) Basic Formulation\n",
    "\n",
    "\n",
    "\n",
    "### 1.1.1 \n",
    "Please derive the formulation of $p(y=-1|\\mathbf{x})$. \n",
    "\n",
    "### 1.1.2 \n",
    "Please show that  $p(y|\\mathbf{x})=\\displaystyle\\frac{1}{1+e^{-y (\\mathbf w^T \\mathbf{x} + b)}}$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1.2 (17 points) Derive Gradient\n",
    "\n",
    "Given a training dataset $S_\\text{training} = \\{(\\mathbf{x}_i, y_i)\\}, i=1,\\ldots,n\\}$, we wish to optimize the negative log-likelihood loss $\\mathcal{L}(\\mathbf w, b)$ of the logistic regression model defined above:\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\mathbf w, b)=-\\sum_{i=1}^{n} \\ln p_i \n",
    "\\end{equation}\n",
    "\n",
    "where $p_i = p(y_i|\\mathbf{x}_i)$. The optimal weight vector $\\mathbf{w}$ and bias $b$ are used to build the logistic regression model:\n",
    "\\begin{equation}\n",
    "\\mathbf{w}^*, b^* = \\arg\\min_{\\mathbf{w}, b}\\mathcal{L}(\\mathbf{w}, b)\n",
    "\\end{equation}\n",
    "In this problem, we attempt to obtain the optimal parameters $\\mathbf{w}^*$ and $b^*$ by using a standard gradient descent algorithm. \n",
    "\n",
    "### 1.2.1 \n",
    "Please show that $\\displaystyle\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial \\mathbf{w}} = -\\sum_{i=1}^n (1-p_{i}) y_i\\mathbf{x}_i$. \\\\\n",
    "\n",
    "\n",
    "### 1.2.2\n",
    "\n",
    "Please show that $\\displaystyle\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial b} = -\\sum_{i=1}^n (1-p_{i}) y_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background for Problems 2 - 7\n",
    "\n",
    "We've already discussed the basic formulation of logistic regression in Problem 1. \n",
    "\n",
    "We've seen that it optimizes the log loss  on a training datase using gradient descent. In the example below log loss is augmented by an $L_2$ regularization term, but it could be an $L_1$, an elasticnet mixture of the two, or no regularization at all.\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\mathbf w, b)= -C \\sum_{i=1}^{n} \\ln p(y_i|\\mathbf{x}_i) + \\frac{1}{2} {\\lVert \\mathbf{w} \\rVert_2}^2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}^*, b^* = \\arg\\min_{\\mathbf{w}, b}\\mathcal{L}(\\mathbf{w}, b)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial \\mathbf{w}} = -C \\sum_{i=1}^n (1-p_{i}) y_i\\mathbf{x}_i + \\mathbf{w}, \\quad\\quad\n",
    "\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial b} = -C \\sum_{i=1}^n (1-p_{i}) y_i.\n",
    "\\end{align}\n",
    "\n",
    "The log loss function is convex. The regularization term is also convex. The sum of two convex functions is also convex. Therefore gradient descent should be able to find the global optimum.\n",
    "\n",
    "Once the gradient descent has converged, that is the delta is vanishingly small (under a tolerance threshold), then it has arrived at a flat (within tolerance) spot which must be the global minimum.  An implementation like in scikit-learn will continue gradient descent until it reaches convergence or until reaches a maximum number of iterations. The iterations limit prevents it from spending forever trying to solve an unsolvable problem like when classes are not linearly seperable.  There are many differen numeric solver algorithms that can do this task, each with their own pros and cons. \n",
    "\n",
    "We will explore all this below.\n",
    "\n",
    "## Scikit-learn logistic regression\n",
    "\n",
    "Let's use sklearn's LogisticRegression class on some not quite seperable binary classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set_style('white')\n",
    "\n",
    "from sklearn.datasets import load_digits, make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler \n",
    "from sklearn.preprocessing import RobustScaler, Normalizer, QuantileTransformer, PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=1972, cluster_std=1.2)\n",
    "sns.scatterplot(X[:, 0], X[:, 1], hue=y, s=75)\n",
    "sns.despine()\n",
    "xl = plt.xlim()\n",
    "yl = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_probability(model, xlim=(-5,5), ylim=(-5,5)):\n",
    "    ax = plt.gca()\n",
    "    cm = plt.cm.RdBu_r\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    Z = model.predict_proba(xy)[:, 1].reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary\n",
    "    ax.contour(X, Y, P, colors='k', levels=[0])\n",
    "    \n",
    "    # plot probabilities\n",
    "    CS = ax.contourf(X, Y, Z, cmap=cm, alpha=.8)\n",
    "    fig = plt.gcf();\n",
    "    cbar = fig.colorbar(CS)\n",
    "    cbar.ax.set_title('logit probability')\n",
    "    \n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is [scikit-learn's documentation on LogisticRegression()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). \n",
    "\n",
    "## 2. Basic use of LogisticRegression()\n",
    "\n",
    "Enter your code at the prompts below corresponding to the problem numbers here.\n",
    "\n",
    "### 2.1  (5 points)\n",
    "Use the docs above to create a classifier with no regularization. \n",
    "\n",
    "### 2.2  (5 points)\n",
    "Get the accuracy of the classifier on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = #### (2.1) YOUR CODE GOES HERE ##### \n",
    "\n",
    "model.fit(X,y)\n",
    "\n",
    "acc =  #### (2.2) YOUR CODE GOES HERE ##### \n",
    "\n",
    "plot_decision_probability(model, xlim=xl, ylim=yl)\n",
    "sns.scatterplot(X[:, 0], X[:, 1], hue=y, s=75);\n",
    "\n",
    "print(f'Training set accuracy {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model,X,y,normalize='true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do it again below for the regularized version.\n",
    "\n",
    "## 3. Use L2 regularization with LogisticRegression()\n",
    "Enter your code at the prompts below corresponding to the problem numbers here. Again you may find [scikit-learn's documentation on LogisticRegression()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) helpful\n",
    "\n",
    "\n",
    "\n",
    "### 3.1 (5 points)\n",
    "\n",
    "Create a classifier with $L2$ regularization with C=0.001 \n",
    "\n",
    "### 3.2 (2 points)\n",
    "\n",
    "Get the accuracy of the classifier on the training data as you did in 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l2 =  #### (3.1) YOUR CODE GOES HERE ##### \n",
    "model_l2.fit(X,y)\n",
    "acc = #### (3.2) YOUR CODE GOES HERE #####  \n",
    "\n",
    "plot_decision_probability(model_l2, xlim=xl, ylim=yl)\n",
    "sns.scatterplot(X[:, 0], X[:, 1], hue=y, s=75);\n",
    "\n",
    "print(f'Training set accuracy {acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model_l2,X,y,normalize='true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (10 points) Describe the difference in the no regularization vs the regularized classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to do some cross-validation to find the best possible value of C to solve a much more complex problem.\n",
    "\n",
    "The digits dataset is 1797 examples of handwritten digits 0-9, digitized into tiny 8x8 (64) pixel images.  We are going to turn this into a binary classification problem by putting digits 0-4 as the positive class and digits 5-9 as the negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# classify small against large digits\n",
    "y = (y > 4).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heres some examples of these super low resolution handwritten digits\n",
    "fig, axs = plt.subplots(3, 10, figsize=(10,4))\n",
    "for n, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(X[n,:].reshape(8,8), cmap='gray_r')\n",
    "    ax.axis('off')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode each image we just turn the 8x8 image into a single 64 long vector. That is we pretend that each pixel is just an independent variable from which we are trying to  \n",
    "\n",
    "Below we are going to try out several difference values of C (inside the list `Cvals`).  For each C, we will do 5 fold cross-validation, and see what the mean validation accuracy is across all 5 folds.  The C value with the highest accuracy is our best choice. \n",
    "\n",
    "## 5. Validation curve\n",
    "\n",
    "Enter your code at the prompts below corresponding to the problem numbers here.\n",
    "\n",
    "\n",
    "\n",
    "### 5.1 (5 points)\n",
    "Instantiate the correct logistic regression setup for each C value we test. Note that you may have to adjust the solver's parameters if you encounter a `Convergence Warning`. This means that the numercial solver perfroming gradient descent did not reach a flat enough (delta < tolerance) area before reaching the maximum allowable number of gradient descent steps.  The answer you have is an answer, but it is not a good enough answer because gradient descent didn't finish! You will need to look at the sklearn docs to figure out how to fix this issue.\n",
    "\n",
    "### 5.2 (5 points)\n",
    "Use the kfold cross validation to create two lists: `train` and `holdhout` which have the indices of those elements of the `X` matrix that will be used for the training and holdout (validation) at each iteration (fold of the cross validator)\n",
    "\n",
    "### 5.3 (5 points)\n",
    "Make a plot showing C value on x-axis and cross-validation score on the y-axis. You will need to make the x-axis a log scale so we can see things properly. Some relevant docs are: [pandas.DataFrame.plot](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html) and the [pandas plotting tutorial](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cvals = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6]\n",
    "k_fold = KFold(n_splits=5)\n",
    "\n",
    "results_l2=[]\n",
    "for C in Cvals:\n",
    "    # instantiate a logistic regression with L2 penalty and the proper C value for this iteration of the loop\n",
    "    model = #### (5.1) YOUR CODE GOES HERE ##### \n",
    "    \n",
    "    # collect the predicted y values and true y values of each hold out set\n",
    "    predicteds=[]\n",
    "    trueys=[]\n",
    "    #### (5.2) YOUR CODE GOES HERE ##### \n",
    "        model.fit(X[train],y[train])\n",
    "        predicteds.append( model.predict(X[holdout]) )\n",
    "        trueys.append( y[holdout] )\n",
    "\n",
    "    # this is because we ended up with a list of arrays, one entry per fold\n",
    "    # we need a flat array of all the folds together\n",
    "    predicteds = np.concatenate(predicteds)\n",
    "    trueys = np.concatenate(trueys)\n",
    "    \n",
    "    results_l2.append( [C, accuracy_score(trueys,predicteds)] ) \n",
    "\n",
    "results_l2 = pd.DataFrame( results_l2, columns=['C', 'accuracy'])\n",
    "#### (5.3) YOUR CODE GOES HERE ##### \n",
    "\n",
    "results_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oops\n",
    "We have committed one of the fundamental errors of Machine Learning.  We applied a technique without really knowing if it was appropriate to our data. Generally in any ML or data analysis problem the first thing to do is to check out the data in as raw a form as you can. Then do some summary statistics to check out its properties.  Let's fix this problem below\n",
    "\n",
    "Let's see what the raw pixel values are like... Hmmm looks like there are 16 levels of gray, and most measurements are 0. The rest of the gray values are pretty uniformly distributed, except for slightly more 15s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = pd.Series( X.ravel()) # this are just all the pixel values in one lump \n",
    "xvals.hist(bins=16)\n",
    "xvals.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the mean values per pixel are mostly zeros, and a bit bimodal looking. Whatever the exact distribution, the important part is that there are definitely pixels with very different average brightnesses.  The difference range over a factor of 10x.  Which is maybe just big enough to be worried about in terms of the algorithm preferring class-predicting large magnitude pixels to similarly good at class-predicting small magnitude pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmm, lets see if the mean values for each pixel (vector element) are roughly the same\n",
    "# maybe there are some pixels which tend to be darker on average than others?\n",
    "xvals_per_pixel = pd.DataFrame(X)\n",
    "xvals_per_pixel.mean().hist()\n",
    "xvals_per_pixel.mean().describe()\n",
    "# yep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see very much the same pattern with standard deviation, but even moreso. There are pixels with brightness that is highly variable and pixels which are always about the same.  This also about an order of magnitude, and therefore maybe just big enough for us to worry about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmm, lets see if the standard deviation of the values for each pixel (vector element) are roughly the same\n",
    "# maybe there are some pixels which tend to be much more varied in their brightness on than others?\n",
    "xvals_per_pixel.std().hist()\n",
    "xvals_per_pixel.std().describe()\n",
    "# yep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now these differences are only a factor of 10 between different variables. But we do know that Logistic Regression is an algorithm that can tend to rely more on variables that have larger magnitude than smaller magnitude variables.  The results above indicate that we there probably isn't much to worry about, but we should at least see if rescaling the variables will change our results.\n",
    "\n",
    "## 6. Use standard scaler with L2 regularization to generate a validation curve\n",
    "\n",
    "Enter your code at the prompts below corresponding to the problem numbers here.\n",
    " \n",
    "\n",
    "### 6.1 (5 points)\n",
    "Transform the X variable using [sklearn.preprocessing.StandardScaler()](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "\n",
    "### 6.2 (2 points)\n",
    "Reuse the LogisticRegression call from 5.1\n",
    "\n",
    "### 6.3 (2 points)\n",
    "Reuse the k-folds for loop from 5.2, but change X to Xsc!\n",
    "\n",
    "### 6.4 (2 points)\n",
    "Reuse the plot command from 5.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsc =  #### (6.1) YOUR CODE GOES HERE #####\n",
    "\n",
    "\n",
    "Cvals = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6]\n",
    "k_fold = KFold(n_splits=5)\n",
    "\n",
    "results_l2sc=[]\n",
    "for C in Cvals:\n",
    "    # instantiate a logistic regression with L2 penalty and the proper C value for this iteration of the loop\n",
    "    model = #### (6.2) YOUR CODE GOES HERE ##### \n",
    "    \n",
    "    # collect the predicted y values and true y values of each hold out set\n",
    "    predicteds=[]\n",
    "    trueys=[]\n",
    "    #### (6.3) YOUR CODE GOES HERE ##### Note you better change X to Xsc!!!\n",
    "        model.fit(Xsc[train],y[train])\n",
    "        predicteds.append( model.predict(Xsc[holdout]) )\n",
    "        trueys.append( y[holdout] )\n",
    "\n",
    "    # this is because we ended up with a list of arrays, one entry per fold\n",
    "    # we need a flat array of all the folds together\n",
    "    predicteds = np.concatenate(predicteds)\n",
    "    trueys = np.concatenate(trueys)\n",
    "    \n",
    "    results_l2sc.append( [C, accuracy_score(trueys,predicteds)] ) \n",
    "\n",
    "results_l2sc = pd.DataFrame( results_l2sc, columns=['C', 'accuracy'])\n",
    "#### (6.4) YOUR CODE GOES HERE ##### \n",
    "\n",
    "\n",
    "results_l2sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. L1 normalization with standardized inputs\n",
    "\n",
    "Let's try an L1 normalized LogisticRegression(), and for simplicity we will only consider the case where we use StandardScaler().\n",
    "\n",
    "### 7.1 (5 points)\n",
    "Instantiate the correct model with L1 penalty. Note that you will have to change the solver as the default does not allow L1. Again you may find [scikit-learn's documentation on LogisticRegression()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) helpful. To help us make the results reproducible please add the argument `random_state=42` to your call as well  \n",
    "\n",
    "\n",
    "### 7.2 (2 points)\n",
    "Reuse the k-folds for loop from 5.2, but change X to Xsc!\n",
    "\n",
    "### 7.3 (2 points)\n",
    "Reuse the plot command from 5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Cvals = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6]\n",
    "k_fold = KFold(n_splits=5)\n",
    "\n",
    "results_l1sc=[]\n",
    "for C in Cvals:\n",
    "    # instantiate a logistic regression with L1 penalty and the proper C value for this iteration of the loop\n",
    "    model = #### (7.1) YOUR CODE GOES HERE ##### \n",
    "    \n",
    "    # collect the predicted y values and true y values of each hold out set\n",
    "    predicteds=[]\n",
    "    trueys=[]\n",
    "    #### (7.2) YOUR CODE GOES HERE ##### \n",
    "        model.fit(Xsc[train],y[train])\n",
    "        predicteds.append( model.predict(Xsc[holdout]) )\n",
    "        trueys.append( y[holdout] )\n",
    "\n",
    "    # this is because we ended up with a list of arrays, one entry per fold\n",
    "    # we need a flat array of all the folds together\n",
    "    predicteds = np.concatenate(predicteds)\n",
    "    trueys = np.concatenate(trueys)\n",
    "    \n",
    "    results_l1sc.append( [C, accuracy_score(trueys,predicteds)] ) \n",
    "\n",
    "results_l1sc = pd.DataFrame( results_l1sc, columns=['C', 'accuracy'])\n",
    "#### (7.3) YOUR CODE GOES HERE ##### \n",
    "\n",
    "\n",
    "results_l1sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK lets put all 3 models validation curves together and take a look at how the different model/feature setups we've tried have changed what happened "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_l1sc['model']='L1,scaled'\n",
    "results_l2['model']='L2,raw'\n",
    "results_l2sc['model']='L2,scaled'\n",
    "results_all=pd.concat([results_l1sc,results_l2,results_l2sc])\n",
    "\n",
    "sns.lineplot(data=results_all, x='C', y='accuracy', hue='model')\n",
    "plt.ylim(0.7,0.9)\n",
    "plt.xscale('log')\n",
    "\n",
    "results_all.groupby('model').apply( lambda x: \n",
    "                                   x.sort_values(by='accuracy',ascending=False)\n",
    "                                   .iloc[0]\n",
    "                                   .drop('model')\n",
    "                                   .rename({'accuracy':'max accuracy'})\n",
    "                                  ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. (10 points) Compare the different model forms you've tried\n",
    "Write a short paragraph about the difference in validation curves between unscaled L2, scaled L2, and scaled L1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type your comparison here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing note\n",
    "\n",
    "We only tried the StandardScaler() but it is worth your time to [investigate all the different scaling options available in sklearn](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
